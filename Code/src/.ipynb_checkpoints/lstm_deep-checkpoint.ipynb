{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verbose: True\n",
      "normal: True\n",
      "nhidden2: 300\n",
      "win: 3\n",
      "layer_norm: False\n",
      "savemodel: False\n",
      "seed: 345\n",
      "emb_dimension: 90\n",
      "nepochs: 40\n",
      "minibatch_size: 4978\n",
      "nhidden: 200\n",
      "decay: True\n",
      "experiment: deep\n",
      "lr: 0.1\n",
      "folder: ../result\n",
      "... loading the dataset\n",
      "Sentences in train: 4978, Words in train: 56590\n",
      "Sentences in test: 893, Words in test: 9198\n",
      "... building the model\n",
      "... training\n",
      "NEW BEST: epoch 0, minibatch 1/1, best test F1: 91.850\n",
      "\n",
      "NEW BEST: epoch 2, minibatch 1/1, best test F1: 92.740\n",
      "\n",
      "NEW BEST: epoch 4, minibatch 1/1, best test F1: 93.660\n",
      "NEW BEST: epoch 5, minibatch 1/1, best test F1: 94.140\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.05)\n",
      "NEW BEST: epoch 16, minibatch 1/1, best test F1: 94.370\n",
      "NEW BEST: epoch 17, minibatch 1/1, best test F1: 94.580\n",
      "NEW BEST: epoch 18, minibatch 1/1, best test F1: 94.790\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.025)\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.0125)\n",
      "\n",
      "('Decay happened. New Learning Rate:', 0.00625)\n",
      "NEW BEST: epoch 31, minibatch 1/1, best test F1: 94.930\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For Deep LSTM we have got accuracy of 94.93 which is very close 95.05 of paper. If we have more time we can try\n",
    "to increase number of epochs and see if we can achive better result\"\"\"\n",
    "import lstm\n",
    "import imp\n",
    "imp.reload(lstm)\n",
    "\n",
    "param = {\n",
    "    'experiment':'deep',\n",
    "    'lr': 0.1,\n",
    "    'verbose': True,\n",
    "    'decay': True,\n",
    "    'win': 3,\n",
    "    'nhidden': 200,\n",
    "    'nhidden2':300,\n",
    "    'seed': 345,\n",
    "    'emb_dimension': 90,\n",
    "    'nepochs': 40,\n",
    "    'savemodel': False,\n",
    "    'normal': True,\n",
    "    'layer_norm': False,\n",
    "    'minibatch_size':4978,\n",
    "    'folder':'../result'}\n",
    "\n",
    "lstm.test_lstm(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
